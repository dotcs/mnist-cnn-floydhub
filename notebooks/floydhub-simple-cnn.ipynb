{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST - CNN on FloydHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Thanks to the dockerized setup, input and output directories are the same as on FloydHub\n",
    "INPUT_ROOT = os.path.abspath('/input/')\n",
    "OUTPUT_ROOT = os.path.abspath('/output/')\n",
    "assert os.path.exists(INPUT_ROOT)\n",
    "assert os.path.exists(OUTPUT_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(os.path.join(INPUT_ROOT, 'train.csv'))\n",
    "df_test = pd.read_csv(os.path.join(INPUT_ROOT, 'test.csv'))\n",
    "assert df_train.shape == (42000, 785)\n",
    "assert df_test.shape == (28000, 784)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = df_train.ix[:,1:].values / 255\n",
    "data_y = df_train[['label']].values.T.flatten()\n",
    "assert data_x.shape == (42000, 784)\n",
    "assert data_y.shape == (42000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_scaler = OneHotEncoder()\n",
    "data_y = target_scaler.fit_transform(data_y.reshape((-1, 1))).todense()\n",
    "assert data_y.shape == (42000, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    data_x.astype(np.float32),\n",
    "    data_y.astype(np.float32),\n",
    "    train_size=(5./7)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare kaggle test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_test = df_test.values / 255\n",
    "assert data_test.shape == (28000, 784)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define CNN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "training_iters = 100000\n",
    "batch_size = 56\n",
    "display_step = 10\n",
    "\n",
    "dim_x_y = 28\n",
    "dim_depth = 1\n",
    "n_input = dim_x_y*dim_x_y\n",
    "n_classes = 10\n",
    "keep_prob = 0.75\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "input_layer = tf.reshape(x, shape=[-1, dim_x_y, dim_x_y, dim_depth])\n",
    "\n",
    "conv1 = tf.layers.conv2d(input_layer, filters=64, kernel_size=[3,3], padding=\"same\")\n",
    "conv2 = tf.layers.conv2d(conv1, filters=64, kernel_size=[3,3], padding=\"same\")\n",
    "max_pool1 = tf.layers.max_pooling2d(conv2, pool_size=[2,2], strides=[2,2])\n",
    "conv3 = tf.layers.conv2d(max_pool1, filters=128, kernel_size=[3,3], padding=\"same\")\n",
    "conv4 = tf.layers.conv2d(conv3, filters=128, kernel_size=[3,3], padding=\"same\")\n",
    "max_pool2 = tf.layers.max_pooling2d(conv4, pool_size=[2,2], strides=[2,2])\n",
    "max_pool2 = tf.reshape(max_pool2, shape=[-1,7*7*128])\n",
    "\n",
    "dense1 = tf.layers.dense(max_pool2, units=1024, activation=tf.nn.relu)\n",
    "dropout = tf.nn.dropout(dense1, keep_prob=keep_prob)\n",
    "logits = tf.layers.dense(dropout, units=10, activation=tf.nn.relu)\n",
    "\n",
    "pred = tf.nn.softmax(logits)\n",
    "\n",
    "cost = tf.reduce_mean(tf.losses.softmax_cross_entropy(y,logits))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "pred_numbers = tf.argmax(pred, 1)\n",
    "correct_pred = tf.equal(pred_numbers, tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Started optimization')\n",
    "step = 1\n",
    "while step * batch_size < training_iters:\n",
    "    batch_index = np.random.permutation(x_train.shape[0])\n",
    "    batch_x = x_train[batch_index,:][:batch_size]\n",
    "    batch_y = y_train[batch_index,:][:batch_size]\n",
    "    \n",
    "    sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
    "    \n",
    "    if step % display_step == 0:\n",
    "        loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x, y: batch_y})\n",
    "        print('Iter: {}, Minibatch loss: {:.6f}, Training accuracy: {:.5f}'\n",
    "            .format(str(step*batch_size), loss, acc))\n",
    "        \n",
    "    step += 1\n",
    "    \n",
    "print ('Optimization Finished!')\n",
    "\n",
    "# save model\n",
    "save_path = saver.save(sess, os.path.join(OUTPUT_ROOT, 'model.ckpt'))\n",
    "print('Model saved in file: %s' % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = np.zeros(data_test.shape[0])\n",
    "\n",
    "for i in range(int(result.shape[0] / 100)):\n",
    "    i_slice = slice(i*100, (i+1)*100)\n",
    "    prediction = sess.run(pred_numbers, feed_dict={x: data_test[i_slice]})\n",
    "    result[i_slice] = prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = pd.DataFrame(result, columns=['Label'], index=df_test.index+1, dtype=np.int64)\n",
    "df_result = df_result.reset_index()\n",
    "df_result.columns = ['ImageId', 'Label']\n",
    "df_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results\n",
    "result_filepath = os.path.join(OUTPUT_ROOT, 'prediction.csv')\n",
    "df_result.to_csv(result_filepath, index=False)\n",
    "print('Results written to {}'.format(result_filepath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
